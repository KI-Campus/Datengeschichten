{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f53966",
   "metadata": {},
   "source": [
    "# Quantitative Textanalyse: *Wordclouds*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae67d67",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Schön, das Du wieder dabei bist! In diesem *Notebook* wollen wir uns damit beschäftigen, wie man sogannte *Wordclouds* oder *Wortwolken* in R erzeugen kann. \n",
    "\n",
    "Hierzu nutzen wir zunächst den Text der Wikipediaseite der Stadt Bielefeld (https://de.wikipedia.org/wiki/Bielefeld)! Mit dieser *Wordcloud* können wir visualisieren, welche Worte auf der Wikipediaseite besonders häufig genannt werden. Oder anders ausgedrückt, wie die **Stadt Bielefeld** auf Wikipedia  dargestellt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d927188",
   "metadata": {},
   "source": [
    "## Was sind Wordclouds und für welche Informationen kann man sie gut nutzen?\n",
    "\n",
    "<img width=\"440\" align = \"right\" hspace=\"40\" src=\"WordcloudDataStorytelling.png\"/> \n",
    "\n",
    "\n",
    "<div style=\"text-align: left\" style=“line-height: 150%“>\n",
    "<p></p>  \n",
    "<p> <b>Wortwolken</b> bzw. <b>WordClouds</b> ermöglichen es Texte visuell darzustellen. Die Wortwolke rechts visualisiert unterschiedliche Definitionen des Begriffs <i>Data Storytelling</i>. In unserem Beispiel schauen wir uns hierzu später den <b>Wikipedia Artikel der Stadt Bielefeld</b> an.</p> \n",
    "   <p> Es lassen sich auch ganz andere Textformen oder auch Textelemente mit Hilfe von WordClouds visualisieren, egal, ob <i>Wikipedia Artikel</i>, <i>PDF-Dokumente</i>, <i>Twitter-<br>Feeds</i> oder ganze <i>Bücher</i>, fast jede Textform ist nutzbar.</p> \n",
    "    \n",
    "\n",
    "Die Wörter aus denen eine WordCloud besteht sind z.B. unterschiedlich **groß**. Je größer ein Wort in der Wortwolke ist, desto häufiger wird es im zugrundeliegenden Text genutzt. Das größte Wort stellt beispielswiese den *Themenschwerpunkt* eines Textes heraus. Zusammen mit weiteren, etwas kleinern, aber immer noch groß dargestellten Worten der WordCloud lassen sich so vielleicht schon *thematische Zusammenhänge* erkennen oder *besonders relevante Themen* eines Textes.\n",
    "</div>      \n",
    "\n",
    "Neben der *Größe* gibt es noch andere Möglichkeiten die Häufigkeit, d.h. wie oft Wörter in einem Text vorkommen zu visualisieren. Beispielsweise über eine **farbliche Kennzeichnung**. Das sieht dann z.B. so aus, dass das meistgenutzte Wort in Rot hervorgehoben wird (natürlich sind auch andere Farben möglich oder die Farbzuordnung wird dem Zufall überlassen).\n",
    "\n",
    "Hierin liegt die *Stärke von Wortwolken*, ohne dass ganze Bücher gelesen werden müssen oder ohne den ganzen Wikipedia Artikel der Stadt Bielefeld zu lesen, kann mit dieser Visualisierungsform zügig ein erster Eindruck über das relevante Thema bzw. die relevanten Themen eines Textelements gewonnen werden.\n",
    "\n",
    "Gerade wenn WordClouds in Präsentationen genutzt werden, bieten diese den Vorteil, mehr Aufmerksamkeit auf sich zu ziehen als eine schlichte Auflistung von thematischen Schwerpunkten eines Textes.\n",
    "\n",
    "In R lassen sich die Möglichkeiten zur Visualisierung einer Wortwolke ganz einfach umsetzten. Egal, ob über die *Größe der Wörter* oder eine *farbliche Unterscheidung* - auch eine Kombination verschiedener Visualisierungsformen ist dabei möglich. R bietet uns die Möglichkeit selbst eine WordCloud zu erstellen.\n",
    "\n",
    "Die erste Frage die sich jetzt stellt ist: ***wie kann ich einen Text in R einlesen?***\n",
    "\n",
    "Hierfür gibt es verschiedene Lösungen, abhängig davon in welcher Form der Text vorliegt -über entsprechenden Pakete und Befehle.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f89af01",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "**Gut zu wissen**: Wir werden Dir in diesem Notebook auch einige Vorgehensweisen zeigen, wie z.B. ein *R-Projekt* anzulegen, die dann besonders wichtig werden, wenn Du mit einer eigenen *RStudio-Installation* und nicht mit einem *Jupyter Notebook* arbeitest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca18325",
   "metadata": {},
   "source": [
    "# WordCloud in R erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3b1ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## R-Projekt für die WordCloud anlegen\n",
    "Als erstes erstellen wir in RStudio ein neues *Projekt*. Dazu klickt man auf \"File\", dann auf \"New Project...\" und anschließend auf \"New Directory\". Als \"Project Type\" wählt man \"New Project\". Nun geben wir unserem Projekt einen Namen und wählen den Speicherort auf unserem Rechner aus. Den Speicherort bzw. den Speicherpfad merken wir uns, diesen brauchen wir gleich. R erstellt nun ein neues Projekt.\n",
    "\n",
    "Die Texte, die wir als Dateien vorliegen haben und für die WordCloud nutzen wollen kopieren wir nun in diesen Ordner. Den Wikipedia Eintrag der Stadt Bielefeld haben wir als Textdatei vorliegen, das Dateiformat lässt sich an der Dateiendung .txt erkennen. (Falls die Dateiendung bei Dir  nicht angezeigt wird, kannst Du auch mit einem Rechtsklick auf die Datei unter Eigenschaften den Dateityp nachsehen). Jetzt befindet sich die Datei im Projektordner und wir können beginnen das *Script zum Generieren einer WordCloud* zu erstellen.Du kannst auch ein bestehendes Projekt nutzen, dann muss die Textdatei einfach in den entsprechenden Ordner eingefügt werden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a7d23",
   "metadata": {},
   "source": [
    "## Script für die WordCloud: Pakete installieren und laden\n",
    "Im ersten Schritt installieren wir alle Pakete, die Befehle und Funktionen enthalten, die wir benötigen. Da die **packages** in unserer Jupyter Notebook Umgebung bereits vorintstalliert sind, sind die packages hier auskommentiert. In R selbst müsstest Du sie aber bei der Erstbenutzung zunächst installieren.\n",
    "\n",
    "Außerdem müssen wir diese Pakte sozusagen in R noch \"aktivieren\". Das erfolgt mit dem *library* Befehl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installieren der Pakete, die zum  Erstellen einer WordCloud benötigt werden\n",
    "\n",
    "#install.packages(\"tm\")\n",
    "#install.packages(\"SnowballC\")\n",
    "#install.packages(\"wordcloud\")\n",
    "#install.packages(\"wordcloud2\")\n",
    "#install.packages(\"RColorBrewer\")\n",
    "#install.packages(\"stringi\")\n",
    "#install.packages(\"Rcpp\")\n",
    "\n",
    "\n",
    "# laden der benötigten Pakete\n",
    "\n",
    "library(\"tm\")\n",
    "library(\"SnowballC\")\n",
    "library(\"wordcloud\")\n",
    "library(\"RColorBrewer\")\n",
    "library(\"stringi\")\n",
    "library(\"Rcpp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df922617",
   "metadata": {},
   "source": [
    "## Script für die WordCloud: Text einlesen\n",
    "Wir erstellen zunächst ein Objekt \"Wiki-Bielefeld\". Diesem Objekt weisen wir unseren Text als Inhalt zu. Hierzu lesen wir den Text in R ein. R liest mit Hilfe des \"stri_read_lines\" Befehls den Text als Ganzes ein und teilt den Text dabei in einzelne Textzeilen auf. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3923a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Textes Zeile für Zeile\n",
    "\n",
    "Wiki_Bielefeld <- stri_read_lines(\"Bielefeld_Wikipedia_Eintrag.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc5807a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Ausprobieren\n",
    "\n",
    "Las Dir doch einmal das erstellte Objekt \"Wiki_Bielefeld\" ausgeben. Dann kannst Du die Struktur sehen mit der R den Text aufteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a9a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wiki_Bielefeld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812350bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud: Textkorpus erstellen\n",
    "In diesem Schritt fassen wir alle Texte, die wir in R mit den ersten drei Schritten eingelesen haben, zu einem **Textkorpus** zusammen. Bei unserem Beispiel handelt sich dabei nur um die Textdatei mit dem *Wikipedia Eintrag der Stadt Bielefeld*. Wenn wir zum Beispiel eine WordCloud zu allen Städten mit mehr als 100.000 Einwohner:innen in OWL erstellen wollen, würden wir z.B. auch die Wikipedia Artikel der Städte Gütersloh und Paderborn zum Textkorpus hinzufügen, nachdem wir sie für R lesbar gemacht haben. Den so zusammengefassten Korpus weisen wir dem Objekt \"docs\" zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten als Corpus laden, auch für mehrere Dateien ist das notwendig\n",
    "\n",
    "docs <- Corpus(VectorSource(Wiki_Bielefeld))\n",
    "\n",
    "\n",
    "#Hier kannst Du Dir den erstellten Daten-Corpus noch einmal mit ansehen, \n",
    "# gerade bei mehreren Dateien, die zu einem Daten-Corpus zusammengefasst werden ist das ratsam\n",
    "\n",
    "inspect (docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a954142",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud: Matrix anlegen und sortierten Datensatz erstellen\n",
    "\n",
    "Mit Hilfe des \"docs\" Objektes können wir nun eine Matrix mit den Wörtern aus unseren Texten anlegen. So wird die Frequenz bzw. die Häufigkeit mit der die Wörter in unserem Texten vorkommt zählbar gemacht. Außerdem werden die Wörter nach der Häufigkeit ihres Vorkommens sortiert. Beginnend beim Wort mit der höchsten Frequenz wird eine *absteigende Liste* erstellt. Diese sortierte Liste wird anschließend in einen eigenen Datensatz \"d\" geschrieben. Auf diesem basiert dann im nächsten Schritt unsere WordCloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hier wird eine Matrix angelegt, die die Häufigkeit der Begriffe aus den Texten zählbar macht\n",
    "# anlegen der Dokumenten-Matrix und Objektzuweisung\n",
    "\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "m <- as.matrix(dtm)\n",
    "\n",
    "\n",
    "# absteigende Sortierung erstellen beginnend mit dem Wort, dass am meisten vorkommt\n",
    "v <- sort(rowSums(m),decreasing=TRUE)\n",
    "\n",
    "# Datensatz erstellen, der die Wörter mit ihrer Freqenz enthält und nach der vorher angelegten Sortierung sortiert ist\n",
    "d <- data.frame(word = names(v),freq=v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f64db0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud:  WordCloud erstellen\n",
    "Jetzt können wir mit dem Dataframe **\"d\"** als Basis eine Wordcloud erstellen. Als erstes legen wir den ***Seed*** fest, d.h. die WordCloud wird sozusagen immer nach dem gleichen Schema erstellt, der Vorteil hierbei ist, dass so die WordCloud reproduzierbar ist und nicht jedes mal mit einem *neuen* Aussehen erstellt wird.\n",
    "\n",
    "Dann legen wir fest, dass unsere WordCloud aus den Wörtern, die in unserem Dataframe enthalten sind besteht **(d\\\\$words)** und dabei nach der Freqenz der Worthäufigkeiten strukturiert ist **(d\\\\$freq)**. Über **\"min.freq\"** legen wir fest, wie oft  ein Wort mindestens in unserem Datensatz **\"d\"** vorkommen muss, damit es in die WortCloud übernommen wird. Wir können auch begrenzen, aus wievielen Wörtern die WordCloud besteht. \n",
    "\n",
    "Die Anordnung der Wörter kann zufällig erfolgen. Wir lassen unsere Wortwolke aber nicht zufällig erstellen. Dies erfolgt über  **\"random.order=FALSE\".** Mit dem Wert 0.35 (35%) legen wir fest, welcher Anteil an Wörtern  nicht waagerecht dargestellt wird. Der letzte Ausdruck legt die Farbauswahl für die WordCloud fest.\n",
    "\n",
    "*Was fällt Dir beim Erstellen der WordCloud auf?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# festlegen einer \"Vorlage\" für die WordCloud, damit diese reproduziert werden kann und nicht\n",
    "# bei jeder Erstellung zufällig erstellt wird\n",
    "\n",
    "set.seed(1234)\n",
    "\n",
    "# Erstellen der WordCloud - die Wörter mit der höchsten Frequenz (freq) werden am größten dargestellt\n",
    "# die minimale Frequenz damit ein Wort dargestellt wird ist 1\n",
    "# die WordCloud besteht aus max. 100 Wörtern (max.Words)\n",
    "# die Anordnung ist nicht zufällig\n",
    "# rot.per --> legt fest welcher Anteil an Wörtern nicht waagerecht dargestellt wird\n",
    "\n",
    "Wortwolke1 <- wordcloud(words = d$word, freq = d$freq,scale=c(12,1), min.freq = 1,\n",
    "          max.words=100, random.order=FALSE, rot.per=0.35, \n",
    "          colors=brewer.pal(8, \"Dark2\"))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94baf85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Stop Words - \"Stopp-Wörter\" und weitere Aufbereitung\n",
    "\n",
    "**Die erstellte WordCloud ist noch nicht so richtig aussagekräftigt!** \n",
    "\n",
    "Die Wörter, die am häufigsten vorkommen sind ***der, die, das, und*** und ähnliche. Diese Wörter kommen gerade in deutschen Texten sehr oft vor, sind für die WordCloud aber nicht sehr aussagekräftig. Um die Texte, die wir benutzen, von diesen *Füllwörtern* zu befreien gibt es für R *Stop-Word-Listen*. Diese entfernen alle gängigen Wörter, wie eben *der, die, das, und* aus der Liste der Wörter, die in die WordCloud übernommen werden. \n",
    "\n",
    "Hierzu weisen wir unserem erstellten Korpus-Objekt \"docs\" mit einem Befehl einfach diese Stopp-Liste zu und die Wörter der Liste fallen aus der Wertung. Diese *Stop-Word-Listen* gibt es dabei für verschiedene Sprachen.\n",
    "\n",
    "Außer den Stopp-Wörtern ist es außerdem sinnvoll *Zahlen*, (z.B. ausgeschriebene Jahreszahlen wie \"1995\"), *überflüssige Leerzeichen* oder auch die *Zeichensetzung* zu entfernen, um möglichst nur auf die relevanten Informationen und die relevanten Wörter zuzugreifen. \n",
    "\n",
    "Analysiert man beispielsweise einen Twitter-Feed ist es außerdem sinnvoll weitere Sonderzeichen wie \"#\" Hashtags zu entfernen oder auch das \"@\"-Zeichen. Zusätzlich kann man alle Buchstaben als Kleinbuchstaben definieren um ein einheitliches Schriftbild zu erhalten.\n",
    "Diese Bereinigungen weisen wir hierzu nach und nach unserem Korpus-Objekt zu. \n",
    "\n",
    "***Schau Dir die WordCloud an, die nach der Aufbereitung entsteht. Was fällt Dir auf?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Stop Words - \"Stopp-Wörter\" und weitere Aufbereitung###\n",
    "\n",
    "\n",
    "#Stoppwörter entfernen z.B. der, die, das, und usw. Deutsch und Englisch\n",
    "docs <- tm_map(docs, removeWords, stopwords(\"english\"))\n",
    "docs <- tm_map(docs, removeWords, stopwords(\"german\"))\n",
    "\n",
    "\n",
    "#Zahlen entferne\n",
    "docs <- tm_map(docs, removeNumbers)\n",
    "\n",
    "#Überschüssige Leerzeichen entfernen\n",
    "docs <- tm_map(docs, stripWhitespace)\n",
    "\n",
    "#Zeichensetzung entfernen\n",
    "docs <- tm_map(docs, removePunctuation)\n",
    "\n",
    "#Text aufbereiten entfernen von Sonderzeichen\n",
    "toSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\n",
    "docs <- tm_map(docs, toSpace, \"/\")\n",
    "docs <- tm_map(docs, toSpace, \"@\")\n",
    "docs <- tm_map(docs, toSpace, \"\\\\|\")\n",
    "\n",
    "\n",
    "#alle Großbuchstaben werden zu Kleinbuchstaben \n",
    "docs <- tm_map(docs, content_transformer(tolower))\n",
    "\n",
    "\n",
    "\n",
    "#Hier wird eine Matrix angelegt, die die Häufigkeit der Begriffe aus den Texten zählbar macht\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "m <- as.matrix(dtm)\n",
    "v <- sort(rowSums(m),decreasing=TRUE)\n",
    "d <- data.frame(word = names(v),freq=v)\n",
    "\n",
    "\n",
    "#Festlegen einer \"Vorlage\" für die WordCloud, damit diese reproduziert werden kann und nicht\n",
    "#bei jeder Erstellung zufällig erstellt wird\n",
    "set.seed(1234)\n",
    "\n",
    "\n",
    "#Erstellen der WordCloud - die Wörter mit der höchsten Frequenz (freq) werden am größten dargestellt\n",
    "#die minimale Frequenz damit ein Wort dargestellt wird ist 1\n",
    "#die WordCloud besteht aus max. 100 Wörtern (max.Words)\n",
    "#die Anordnung ist nicht zufällig\n",
    "#rot.per --> legt fest welcher Anteil an Wörtern nicht waagerecht dargestellt wird\n",
    "wordcloud2 <- wordcloud(words = d$word, freq = d$freq, scale=c(9,1), min.freq = 1,\n",
    "          max.words=100, random.order=FALSE, rot.per=0.35, \n",
    "          colors=brewer.pal(8, \"Dark2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc49ce9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Eigene Stopwords\n",
    " Wie Du in der zuletzt erstellten WordCloud siehst, reicht je nach Text die Aufbereitung mit den vordefinierten *Stoppwörter-Listen* noch nicht aus und wir finden immer noch Wörter, die unsere WordCloud in ihrer Aussagefähigkeit einschränken. Um auch diese Wörter aus der WordCloud auszuschließen zu können, können wir eigene Wörter als \"Stop Words\" definieren. Diese Liste weisen wir wiederum auch unserem \"docs\" Objekt zu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c155cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Eigene Stop-Words###\n",
    "docs <- tm_map(docs, removeWords, c(\"sowie\", \"wurden\", \"seit\", \"heute\", \"heut\", \n",
    "                \"teil\", \"befindet\", \"weiter\", \"erst\", \"etwa\", \"rund\", \"gibt\", \n",
    "                \"liegt\", \"zwei\", \"statt\", \"weitere\", \"finden\", \"erste\")) \n",
    "\n",
    "\n",
    "\n",
    "#Hier wird eine Matrix angelegt die die Häufigkeit der Begriffe aus den Texten zählbar macht\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "m <- as.matrix(dtm)\n",
    "v <- sort(rowSums(m),decreasing=TRUE)\n",
    "d <- data.frame(word = names(v),freq=v)\n",
    "\n",
    "\n",
    "#Festlegen einer \"Vorlage\" für die WordCloud, damit diese reproduziert werden kann und nicht\n",
    "#bei jeder Erstellung zufällig erstellt wird\n",
    "set.seed(1234)\n",
    "\n",
    "\n",
    "#Erstellen der WordCloud - die Wörter mit der höchsten Frequenz (freq) werden am größten dargestellt\n",
    "#die minimale Frequenz damit ein Wort dargestellt wird ist 1\n",
    "#die WordCloud besteht aus max. 100 Wörtern (max.Words)\n",
    "#die Anordnung ist nicht zufällig\n",
    "#rot.per --> legt fest welcher Anteil an Wörtern nicht waagerecht dargestellt wird\n",
    "wordcloud3 <-wordcloud(words = d$word, freq = d$freq,scale=c(9,1), min.freq = 1,\n",
    "          max.words=100, random.order=FALSE, rot.per=0.35, \n",
    "          colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8322a8fa",
   "metadata": {},
   "source": [
    "## Ausprobieren\n",
    "In der Liste mit den eigenen \"Stop-Words\" fehlen noch ein paar Wörter. Füge die Wörter zur Liste hinzu und lass Dir die neue WordCloud ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2378ba7",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccea241",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "###Eigene Stop-Words###\n",
    "docs <- tm_map(docs, removeWords, c(\"sowie\", \"wurden\", \"seit\", \"heute\", \"heut\",\n",
    "                \"teil\",\"befindet\", \"weiter\", \"erst\", \"etwa\", \"rund\", \"gibt\", \"liegt\",\n",
    "                \"zwei\",\"statt\",  \"weitere\", \"finden\", \"erste\",\"siehe\", \"wurde\",\n",
    "                \"viele\",\"jedoch\",\"mehrere\",\"isbn\")) \n",
    "\n",
    "\n",
    "\n",
    "#Hier wird eine Matrix angelegt die die Häufigkeit der Begriffe aus den Texten zählbar macht\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "m <- as.matrix(dtm)\n",
    "v <- sort(rowSums(m),decreasing=TRUE)\n",
    "d <- data.frame(word = names(v),freq=v)\n",
    "\n",
    "\n",
    "#Festlegen einer \"Vorlage\" für die WordCloud, damit diese reproduziert werden kann und nicht\n",
    "#bei jeder Erstellung zufällig erstellt wird\n",
    "set.seed(1234)\n",
    "\n",
    "\n",
    "#Erstellen der WordCloud - die Wörter mit der höchsten Frequenz (freq) werden am größten dargestellt\n",
    "#die minimale Frequenz damit ein Wort dargestellt wird ist 1\n",
    "#die WordCloud besteht aus max. 100 Wörtern (max.Words)\n",
    "#die Anordnung ist nicht zufällig\n",
    "#rot.per --> legt fest welcher Anteil an Wörtern nicht waagerecht dargestellt wird\n",
    "wordcloud3 <-wordcloud(words = d$word, freq = d$freq, scale=c(9,1), min.freq = 1,\n",
    "          max.words=100, random.order=FALSE, rot.per=0.35, \n",
    "          colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403fb75e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Hinweis\n",
    "Es gibt darüberhinaus die Möglichkeit mit einem weiteren Befehl Wörter auf ihren Wortstamm zu reduzieren. Beispielsweise werden so die Worte \"Bielefeld\" und \"Bielefelder\" zusammengefasst. Das kann je nach Forschungsinteresse oder Fragestellung durchaus nützlich sein. Wir verzichten hier jedoch auf diesen Befehl, dieser führt stellenweise nämlich auch zur \"Verfälschung\" von Wörtern und kürzt unter anderem Wortendungen weg. Den entsprechenden Befehl und die WordCloud die mit ihm ausgegeben wird siehst Du, wenn Du das folgende Skript ausführst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Hinweis###\n",
    "\n",
    "#Wörter auf ihren Wortstamm reduzieren, sodass beispielsweise Bielfelder und Bielefeld zusammengefasst werden\n",
    "docs <- tm_map(docs, stemDocument)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Hier wird eine Matrix angelegt die die Häufigkeit der Begriffe aus den Texten zählbar macht\n",
    "dtm <- TermDocumentMatrix(docs)\n",
    "m <- as.matrix(dtm)\n",
    "v <- sort(rowSums(m),decreasing=TRUE)\n",
    "d <- data.frame(word = names(v),freq=v)\n",
    "\n",
    "\n",
    "#Festlegen einer \"Vorlage\" für die WordCloud, damit diese reproduziert werden kann und nicht\n",
    "#bei jeder Erstellung zufällig erstellt wird\n",
    "set.seed(1234)\n",
    "\n",
    "\n",
    "#Erstellen der WordCloud - die Wörter mit der höchsten Frequenz (freq) werden am größten dargestellt\n",
    "#die minimale Frequenz damit ein Wort dargestellt wird ist 1\n",
    "#die WordCloud besteht aus max. 100 Wörtern (max.Words)\n",
    "#die Anordnung ist nicht zufällig\n",
    "#rot.per --> legt fest welcher Anteil an Wörtern nicht waagerecht dargestellt wird\n",
    "wordcloud(words = d$word, freq = d$freq, scale=c(9,1), min.freq = 1,\n",
    "          max.words=100, random.order=FALSE, rot.per=0.35, \n",
    "          colors=brewer.pal(8, \"Dark2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48d59d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Websites direkt auslesen und eine WordCloud erstellen\n",
    "Mit R lassen sich nicht nur Textdateien auswerten, Du kannst auch direkt auf **Websites** zugreifen und die Texte dort für die Erstellung von WordClouds nutzen. An den Befehlen für die Aufbereitung der Texte ändert sich soweit nichts, es gibt ein paar Ergänzungen, aber den generellen Ablauf kennen wir schon aus der Erstellung der WordCloud auf Basis von Textdokumenten Textdokumenten.\n",
    "\n",
    "Wir benötigen allerdings einige neue zusätzliche R Funktionen und Befehle. Hierzu installieren und laden wir einige neue Pakete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benötigte Pakete\n",
    "library(\"rvest\")\n",
    "library(\"dplyr\")\n",
    "library(\"htmltools\")\n",
    "library(\"tm\")\n",
    "library(\"SnowballC\")\n",
    "library(\"wordcloud\")\n",
    "library(\"wordcloud2\")\n",
    "library(\"RColorBrewer\")\n",
    "library(\"stringi\")\n",
    "library(\"magrittr\") \n",
    "library(\"dplyr\")    \n",
    "library(\"ggplot2\")\n",
    "library(\"xml2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cef82e",
   "metadata": {},
   "source": [
    "## Websites mit R lesen\n",
    "Um Websites mit R zu lesen benötigen wir zunächst die URL bzw. die Adresse der Seite, die wir nutzen möchten. Wir bleiben bei unserem Beispiel dem Wikipedia Eintrag der Stadt Bielefeld.\n",
    "\n",
    "Dieser ist unter der Adresse https://de.wikipedia.org/wiki/Bielefeld zu erreichen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7a51b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## URL Adresse \n",
    "Die URL-Adresse definieren wir als eigenes Objekt, somit müssen wir nicht immer den kompletten Link angeben. Das neuerstellte Objekt ist \"Wiki_Bi_Eintrag\".\n",
    "\n",
    "## HTML und XML einlesen\n",
    "Textdokumente wie .txt-Dateien und Websites unterscheiden sich in Bezug auf ihre Formatierung, da es sich um völlig unterschiedliche Quellen für das \"gleiche\" Textelement handelt. Um die Website mit R einzulesen nutzen wir daher den read_html Befehl um das Format der Website für R nutzbar zu machen. Den Inhalt den wir damit auslesen, weisen wir dem neuen Objekt Wiki_Bi_html zu. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a00527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abrufen des Wikipedia Artikels der Stadt Bielefeld über die URL \n",
    "Wiki_Bi_Eintrag <- \"https://de.wikipedia.org/wiki/Bielefeld\"\n",
    "\n",
    "\n",
    "Wiki_Bi_html <- read_html(Wiki_Bi_Eintrag)\n",
    "\n",
    "Wiki_Bi_html\n",
    "str(Wiki_Bi_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9bbb3",
   "metadata": {},
   "source": [
    "### Kurzer Exkurs: Quellcode\n",
    "Um von einer Website nur den relevanten Text für die **WordCloud** zu nutzen, ist ein Blick in den Quellcode der Website notwendig. Keine Sorge, das klingt komplizierter als es eigentlich ist. \n",
    "\n",
    "Öffne in einem Browser Deiner Wahl (*Firefox, Google Chrome, Opera, Microsoft Edge*) den Wikipedia Eintrag der Stadt Bielefeld. Jetzt klickt mit der rechten Maustaste irgendwo auf die Seite z.B. neben das Wappen von Bielefeld. In dem Kontextmenü, das sich öffnet klickst Du auf \"Untersuchen\". In Eurem Browser öffnet sich jetzt ein neues Menü neben dem Wikipedia Eintrag. Dieses Fenster zeigt an, aus welchen Elementen die Seite besteht. \n",
    "\n",
    "Ganz oben in dem neuen Menü ist ein Button auf dem ein kleiner Mauszeiger zu sehen ist. Wenn Du auf diesen Button klickt leuchtet dieser Blau und ist aktiviert. Sobald Du irgendwo anders draufklickst deaktiviert sich dieser Button wieder. Lass den Button aktiviert und fahr einfach über den Wikipedia-Eintrag ohne etwas anzuklicken. \n",
    "\n",
    "Es leuchten  verschiedene Kästchen Blau auf und teilweise sind die auch noch mit einem Orangen-Rahmen versehen. Wenn Du z.B. den Mauszeiger auf das Wikipedia Logo bewegst ist dieses blau hinterlegt. Außerdem siehst Du unter dem Logo eine kleine Textbox, der Titel dieser Box lautet:\"a.mw-wiki-logo\". Das ist sozusagen die Adresse des Logos auf der Website. \n",
    "Für die WordCloud brauchen wir das Logo oder auch die linke Spalte mit der Sprachauswahl für den Wikipedia Artikel nicht. Wir schauen also unter welcher Adresse auf der Website der Text des Wikipedia Artikels steckt.\n",
    "\n",
    "Wenn Du den Mauszeiger auf die freie Fläche zwischen \"Inhaltsverzeichnis\" und der Tabelle mit den \"Basisdaten\" auf der rechten Seite bewegst, leuchtet der ganze Text blau auf bzw. ist blau hinterlegt.\n",
    "Das ist die für uns relvante Adresse auf der Website des Wikipedia Eintrags der Stadt Bielefeld: \"div.mw.parser-output\".\n",
    "Das \"div\" können wir streichen für das einlesen in R reicht uns der nachstehende Adressteil: **.mw.parser-output.** Mit dieser Adresse aus der Seitenstruktur des Wikipedia-Eintrags können wir nun weiter arbeiten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7254",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Einlesen des relevanten Textelements von einer Website\n",
    "Nachdem wir ermittelt haben, wo genau der Text, der uns interessiert, auf der Webseite zu finden ist, können wir diesen als Text in R einlesen. Hierzu schreiben wir den Text in ein neues Objekt, sodass dieses Objekt nur noch den Text beinhaltet. Dies gelingt uns mit dem folgenden Skript. Anschließend geben wir das Objekt zur Überprüfung aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff62720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen des Inhaltes / Textes --> über den Pfad .mw-parser-output \n",
    "#hierzu ist ein Auslesen des Quellcodes der Internetseite notwendig\n",
    "\n",
    "Wiki_Bi_Text <-(Wiki_Bi_html %>%\n",
    "                  html_node(\".mw-parser-output\") %>%\n",
    "                  html_text())\n",
    "\n",
    "\n",
    "#Ausgabe des Textes zur Überprüfung\n",
    "Wiki_Bi_Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca74c4",
   "metadata": {},
   "source": [
    "## Text Ausgabe\n",
    "Dir ist bestimmt aufgefallen, dass der Text den Du gerade ausgegeben hast noch etwas chaotisch aussieht. Das liegt unter anderem daran, das z.B. auch die Zeilenumbrüche \"\\n\" im Textelemt als Text vermerkt sind.\n",
    "Du kannst Dir mit dem nachfolgenden Befehl den Text \"ausgeschrieben\" ausgeben lassen, dass sieht dann schon deutlich übersichtlicher aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ddb2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ausgabe des Textes in Zeilen, das ist wesentlich übersichtlicher\n",
    "writeLines(Wiki_Bi_Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efe66c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Ausgabe in Textdatei\n",
    "Bevor wir eine WordCloud aus dem Textobjekt \"Wiki_Bi_Text\"  erstellen, speichern wir den Text mit Hilfe von R in einer seperaten Textdatei ab. Diese Datei wird innerhalb des Befehls benannt: \"Wiki_Bi_Download.txt\" und automatisch in unserem Projektordner gespeichert.\n",
    "\n",
    "Dieses Textdatei lesen wir anschließend, wie wir das oben mit der .txt-Datei schon kennengelernt haben, direkt wieder in R ein und können nun, wie oben bereits beschrieben die Anpassungen und Bereinigungen vornehmen, um eine WordCloud zu erstellen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6476af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ausgabe/Exportierung des (Tabllen-)Objektes mit Titel und Beschreibung in einer Textdatei (.txt)\n",
    "\n",
    "write.table(Wiki_Bi_Text, \"Wiki_Bi_Download.txt\", sep=\"\\n\", row.names=FALSE, na=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac19ea",
   "metadata": {},
   "source": [
    "## Erstellen der WordCloud\n",
    "Analog zum Vorgehen zur Textdatei erfolgt jetzt das Aufbereiten der Texte als auch das Erstellen der Wordcloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce1b69",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud: Text einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Einlesen des Textes Zeile für Zeile\n",
    "Wiki_Bielefeld_down <- stri_read_lines(\"Wiki_Bi_Download.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438616c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud: Textkorpus erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b4f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten als Corpus laden \n",
    "docs_down <- Corpus(VectorSource(Wiki_Bielefeld_down))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1752a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Stop Words - \"Stopp-Wörter\" und weitere Aufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5cf965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stoppwörter entfernen z.B. der, die, das, und usw. Deutsch und Englisch\n",
    "docs_down <- tm_map(docs_down, removeWords, stopwords(\"english\"))\n",
    "docs_down <- tm_map(docs_down, removeWords, stopwords(\"german\"))\n",
    "\n",
    "\n",
    "#Zahlen entfernen\n",
    "docs_down <- tm_map(docs_down, removeNumbers)\n",
    "\n",
    "\n",
    "#Überschüssige Leerzeichen entfernen\n",
    "docs_down <- tm_map(docs_down, stripWhitespace)\n",
    "\n",
    "\n",
    "#Zeichensetzung entfernen\n",
    "docs_down <- tm_map(docs_down, removePunctuation)\n",
    "\n",
    "\n",
    "#Text aufbereiten entfernen von Sonderzeichen\n",
    "toSpace <- content_transformer(function (x , pattern ) gsub(pattern, \" \", x))\n",
    "docs_down <- tm_map(docs_down, toSpace, \"/\")\n",
    "docs_down <- tm_map(docs_down, toSpace, \"@\")\n",
    "docs_down <- tm_map(docs_down, toSpace, \"\\\\|\")\n",
    "docs_down <- tm_map(docs_down, toSpace, \"-\")\n",
    "\n",
    "\n",
    "#alle Großbuchstaben werden zu Kleinbuchstaben \n",
    "docs_down <- tm_map(docs_down, content_transformer(tolower))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Definieren von eigenen Stoppwörtern\n",
    "docs_down <- tm_map(docs_down, removeWords, c(\n",
    "  \"wurde\", \"sowie\", \"wurden\", \"seit\", \"heute\", \"heut\", \"teil\", \"befindet\", \"weiter\", \"erst\", \"etwa\", \"rund\", \n",
    "  \"gibt\", \"liegt\", \"zwei\", \"statt\", \"weitere\", \"finden\", \"erste\", \"abgerufen\", \"isbn\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c428074",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud: Matrix anlegen und sortierten Datensatz erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hier wird eine Matrix angelegt die die Häufigkeit der Begriffe aus den Texten zählbar macht\n",
    "dtm_down <- TermDocumentMatrix(docs_down)\n",
    "m_down <- as.matrix(dtm_down)\n",
    "v_down <- sort(rowSums(m_down),decreasing=TRUE)\n",
    "d_down <- data.frame(word = names(v_down),freq=v_down)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d95f7aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Script für die WordCloud:  WordCloud erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Festlegen einer \"Vorlage\" für die WordCloud, damit diese reproduziert werden kann und nicht\n",
    "#bei jeder Erstellung zufällig erstellt wird\n",
    "set.seed(1234)\n",
    "\n",
    "\n",
    "#Erstellen der WordCloud - die Wörter mit der höchsten Frequenz (freq) werden am gröten dargestellt\n",
    "#die minimale Frequenz damit ein Wort dargestellt wird ist 1\n",
    "#die WordCloud besteht aus max. 100 W?rtern (max.Words)\n",
    "#die Anordnung ist nicht zufällig\n",
    "#rot.per --> legt fest welcher Anteil an W?rtern nicht waagerecht dargestellt wird\n",
    "w <- wordcloud(words = d_down$word, freq = d_down$freq,scale=c(9,1), min.freq = 1,\n",
    "          max.words=100, random.order=FALSE, rot.per=0.35, \n",
    "          colors=brewer.pal(8, \"Dark2\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe22d0b7",
   "metadata": {},
   "source": [
    "## Anmerkung zum Unterschied Textdatei und Webdokument\n",
    "Wie Du siehst, sind die WordClouds, die auf den unterschiedlichen Grundlagen, einmal der *Textdatei* und einmal der *abgerufenen Website* nicht komplett identisch. Gerade die abgerufene Website beinhaltet noch andere Textelemte wie z.B. die Einzelnachweise des Artikels oder auch die \"Liste Deutscher Großstädte\". Wir haben also beim Abrufen des Artikels von der Website nicht exakt die gleiche Grundlage, wie bei der Textdatei des Artikels. \n",
    "\n",
    "Dies muss unbedingt bei der Auswertung der beiden WordClouds berücksichtigt werden. Auch die Aufbereitung wird hierdurch beeinflusst. Beispielsweise ist bei den Stop-Words für die Website ein Hinzufügen der Abkürzung \"isbn\" notwendig, ein Wort das beispielsweise in der Textdatei überhaupt nicht vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5516569",
   "metadata": {},
   "source": [
    "# Abschluss\n",
    "Wir haben gesehen, die Erstellung von WordClouds ist auf Grundlage der unterschiedlichsten Text-Grundlagen als auch Formate in denen die Texte vorliegen möglich. Je nach Grundlage und Format ist eine dementsprechende sorgfältige und unterschiedliche Aufbereitung notwendig.\n",
    "\n",
    "Beachtest Du dies und kommunizierst offen und transparent, wie Deine WordCloud entstanden ist und auf welcher Datengrundlage sie beruht, steht einer Verwendung bei der nächsten Visualisierung von Daten nichts mehr im Weg!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "244.067px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
